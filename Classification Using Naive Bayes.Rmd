---
title: "Classification Using Naive Bayes"
author: "Edward Moradian"
date: "14 April 2018"
output:
  html_document: default
  word_document: default
---

Question 1)

1) Step One: Collecting the Data

Read the sms data into the sms data frame
```{r}
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)

```

2) Step Two: Exploring and Preparing the Data

Examine the structure of the sms data
```{r}
str(sms_raw)
```

Convert spam/ham to factor
```{r}
sms_raw$type <- factor(sms_raw$type)
```

Examine the type variable more carefully
```{r}
str(sms_raw$type)
table(sms_raw$type)
```

Build a corpus using the text mining (tm) package
```{r}
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
```

Examine the sms corpus. Print 1st and 2nd text messages. Using inspect() function to receive a summary of specific messages.
```{r}
print(sms_corpus)
inspect(sms_corpus[1:2])

as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
```

Clean up the corpus using tm_map()
```{r}
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
```

Show the difference between sms_corpus and corpus_clean
```{r}
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]]) #all lowercase letters

sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers) # remove numbers
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove stop words
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation
```

Tip: create a custom function to replace (rather than remove) punctuation
```{r}
removePunctuation("hello...world")
replacePunctuation <- function(x) { gsub("[[:punct:]]+", " ", x) }
replacePunctuation("hello...world")
```

Illustration of word stemming
```{r}
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))

sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)

sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace
```

Examine the final clean corpus
```{r}
lapply(sms_corpus[1:3], as.character)
lapply(sms_corpus_clean[1:3], as.character)
```

Create a document-term sparse matrix
```{r}
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE
))

sms_dtm3 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = function(x) { removeWords(x, stopwords()) },
  removePunctuation = TRUE,
  stemming = TRUE
))
```

Compare the result, sparsity = 100% rounded off mostly 0's in sparsity matrix 42,000 1's in 3.7 million words
```{r}
sms_dtm2
sms_dtm3

sms_dtm <-sms_dtm3
sms_dtm
```

2b) Data Preparation - Creating Training and Test Datasets

```{r}
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
```

Save the labels
```{r}
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels  <- sms_raw[4170:5559, ]$type
```

Check that the proportion of spam is similar in train and test data. Both the training data and the test data contain about 13% spam and 87% ham.
```{r}
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
```

Subset the training data into spam and ham groups
```{r}
spam <- subset(sms_raw, type == "spam")
ham  <- subset(sms_raw, type == "ham")
library(wordcloud)
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))

sms_dtm_freq_train <- removeSparseTerms(sms_dtm_train, 0.999) #proportion that is less than .999
sms_dtm_freq_train
```

Indicator features for frequent words, occur at least 5 times
```{r}
findFreqTerms(sms_dtm_train, 5)
```

Save frequently-appearing terms to a character vector
```{r}
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
```

Create DTMs with only the frequent terms.  Use these words as they are more useful for classification than any word that appear once.
```{r}
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
```

Convert counts to a factor - Yes or No
```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
```

Apply() convert_counts() to columns of train/test data
```{r}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
```

3) Step Three: Training a Model on the Data

```{r}
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
```

4) Step Four: Evaluating Model Performance.
According to the confusion matrix, there are 6 observations out of 1390 that were false negative and 30 observations that were false positive.  Accuracy formula returns .97 and therefore the algorithm does give accurate predictions.  The error rate is .03.  153 out of the 1390 observations where true negatives results where the algorithm identified the text as spam when the text was actually spam.  1201 out of 1390 observations were true positives where the algorithm identified the text as ham when the text was actually ham.

```{r}
sms_test_pred <- predict(sms_classifier, sms_test)

head(sms_test_pred)

library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual'))

Accuracy=(153+1201)/(153+1201+30+6)
Accuracy
```

5) Step Five: Improving Model Performance. The model was improved with higher accuracy.  False postive observations were reduced to 28 and false negative observations were reduced to 5.

```{r}
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels,
           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual'))
```

Question 2)

1) Step One: Collecting the Data

```{r}
library(mlbench)
data("HouseVotes84")
```

2) Step Two: Exploring and Preparing the Data

Examine the structure of the sms data
```{r}
str(HouseVotes84)
```

Examine the Class variable more carefully.  267 democrats and 168 republicans.
```{r}
str(HouseVotes84$Class)
table(HouseVotes84$Class)
```

2b) Data Preparation - Creating Training and Test Datasets.  Both the training data and the test data contain about 61% democrat and 39% republican.

```{r}
votes_train<-HouseVotes84[1:326,-1]
votes_test<-HouseVotes84[327:435,-1]

votes_train_labels <- HouseVotes84[1:326, ]$Class 
votes_test_labels<- HouseVotes84[327:435, ]$Class

prop.table(table(votes_train_labels))
prop.table(table(votes_test_labels))
```

3) Step Three: Training a Model on the Data

```{r}
votes_classifier <- naiveBayes(votes_train, votes_train_labels)
```

4) Step Four: Evaluating Model Performance.
According to the confusion matrix, there are 11 observations out of 109 that were false negative and 3 observations that were false positive.  Accuracy formula returns .87 and therefore the algorithm does give accurate predictions.  The error rate is .13.  55 out of the 109 observations where true negatives results where the algorithm identified the voter as democrat when the voter was actually democrat.  40 out of 109 observations were true positives where the algorithm identified the voter as republican when the voter was actually republican.

```{r}
votes_test_pred <- predict(votes_classifier, votes_test)
head(votes_test_pred)

library(gmodels)
CrossTable(votes_test_pred, votes_test_labels,
           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual'))

votes_test_pred

accuracy2=(40+55)/(40+55+3+11)
accuracy2
```


5) Step Five: Improving Model Performance.
The model was not improved by adding a LaPlace number.

```{r}
votes_classifier2 <- naiveBayes(votes_train, votes_train_labels, laplace = 1)
votes_test_pred2 <- predict(votes_classifier2, votes_test)
CrossTable(votes_test_pred2, votes_test_labels,
           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
           dnn = c('predicted', 'actual'))

votes_test_pred2
```

